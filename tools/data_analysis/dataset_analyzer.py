#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ microWakeWord
"""

import os
import json
import yaml
from pathlib import Path
from typing import Dict, List, Any, Tuple
import numpy as np
import librosa
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import pandas as pd


class DatasetAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è microWakeWord"""
    
    def __init__(self, data_dir: str = "/home/microWakeWord_data"):
        self.data_dir = Path(data_dir)
        self.results = {}
    
    def analyze_dataset(self, dataset_path: str) -> Dict[str, Any]:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        dataset_path = Path(dataset_path)
        
        if not dataset_path.exists():
            raise FileNotFoundError(f"–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_path}")
        
        print(f"üîç –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_path.name}")
        
        # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
        basic_stats = self._analyze_basic_stats(dataset_path)
        
        # –ê–Ω–∞–ª–∏–∑ –∞—É–¥–∏–æ —Ñ–∞–π–ª–æ–≤
        audio_stats = self._analyze_audio_files(dataset_path)
        
        # –ê–Ω–∞–ª–∏–∑ —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º
        spectrogram_stats = self._analyze_spectrograms(dataset_path)
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        quality_stats = self._analyze_data_quality(dataset_path)
        
        # –°–≤–æ–¥–∫–∞
        summary = self._create_summary(basic_stats, audio_stats, spectrogram_stats, quality_stats)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._save_results(dataset_path.name, {
            'basic_stats': basic_stats,
            'audio_stats': audio_stats,
            'spectrogram_stats': spectrogram_stats,
            'quality_stats': quality_stats,
            'summary': summary
        })
        
        return summary
    
    def _analyze_basic_stats(self, dataset_path: Path) -> Dict[str, Any]:
        """–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        print("üìä –ê–Ω–∞–ª–∏–∑ –±–∞–∑–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...")
        
        # –ü–æ–¥—Å—á–µ—Ç —Ñ–∞–π–ª–æ–≤
        audio_files = list(dataset_path.rglob("*.wav"))
        spectrogram_files = list(dataset_path.rglob("*.npy"))
        
        # –†–∞–∑–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤
        total_size = sum(f.stat().st_size for f in audio_files)
        
        # –¢–∏–ø—ã —Ñ–∞–π–ª–æ–≤
        file_types = Counter(f.suffix for f in audio_files)
        
        return {
            'total_files': len(audio_files),
            'spectrogram_files': len(spectrogram_files),
            'total_size_bytes': total_size,
            'total_size_mb': total_size / (1024 * 1024),
            'file_types': dict(file_types),
            'dataset_path': str(dataset_path)
        }
    
    def _analyze_audio_files(self, dataset_path: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∞—É–¥–∏–æ —Ñ–∞–π–ª–æ–≤"""
        print("üéµ –ê–Ω–∞–ª–∏–∑ –∞—É–¥–∏–æ —Ñ–∞–π–ª–æ–≤...")
        
        audio_files = list(dataset_path.rglob("*.wav"))
        
        if not audio_files:
            return {'error': '–ê—É–¥–∏–æ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã'}
        
        # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤—ã—Ö 100 —Ñ–∞–π–ª–æ–≤ (–¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
        sample_files = audio_files[:100]
        
        durations = []
        sample_rates = []
        channels = []
        
        for file_path in sample_files:
            try:
                y, sr = librosa.load(file_path, sr=None)
                durations.append(len(y) / sr)
                sample_rates.append(sr)
                channels.append(1 if y.ndim == 1 else y.shape[0])
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞ {file_path}: {e}")
        
        return {
            'sample_size': len(sample_files),
            'total_files': len(audio_files),
            'duration_stats': {
                'mean': np.mean(durations),
                'std': np.std(durations),
                'min': np.min(durations),
                'max': np.max(durations)
            },
            'sample_rate_stats': {
                'unique_rates': list(set(sample_rates)),
                'most_common_rate': Counter(sample_rates).most_common(1)[0][0]
            },
            'channel_stats': {
                'unique_channels': list(set(channels)),
                'most_common_channels': Counter(channels).most_common(1)[0][0]
            }
        }
    
    def _analyze_spectrograms(self, dataset_path: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º"""
        print("üìà –ê–Ω–∞–ª–∏–∑ —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º...")
        
        spectrogram_files = list(dataset_path.rglob("*.npy"))
        
        if not spectrogram_files:
            return {'error': '–°–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã'}
        
        # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤—ã—Ö 50 —Ñ–∞–π–ª–æ–≤
        sample_files = spectrogram_files[:50]
        
        shapes = []
        values_stats = []
        
        for file_path in sample_files:
            try:
                spectrogram = np.load(file_path)
                shapes.append(spectrogram.shape)
                values_stats.append({
                    'mean': np.mean(spectrogram),
                    'std': np.std(spectrogram),
                    'min': np.min(spectrogram),
                    'max': np.max(spectrogram)
                })
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã {file_path}: {e}")
        
        return {
            'sample_size': len(sample_files),
            'total_files': len(spectrogram_files),
            'shape_stats': {
                'unique_shapes': list(set(shapes)),
                'most_common_shape': Counter(shapes).most_common(1)[0][0] if shapes else None
            },
            'value_stats': {
                'mean_of_means': np.mean([s['mean'] for s in values_stats]),
                'std_of_stds': np.std([s['std'] for s in values_stats]),
                'overall_min': min([s['min'] for s in values_stats]),
                'overall_max': max([s['max'] for s in values_stats])
            }
        }
    
    def _analyze_data_quality(self, dataset_path: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print("üîç –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        issues = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤
        empty_files = []
        for file_path in dataset_path.rglob("*"):
            if file_path.is_file() and file_path.stat().st_size == 0:
                empty_files.append(str(file_path))
        
        if empty_files:
            issues.append(f"–ù–∞–π–¥–µ–Ω–æ {len(empty_files)} –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        corrupted_files = []
        for file_path in dataset_path.rglob("*.wav"):
            try:
                librosa.load(file_path, sr=None)
            except Exception:
                corrupted_files.append(str(file_path))
        
        if corrupted_files:
            issues.append(f"–ù–∞–π–¥–µ–Ω–æ {len(corrupted_files)} –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –∞—É–¥–∏–æ —Ñ–∞–π–ª–æ–≤")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º
        corrupted_spectrograms = []
        for file_path in dataset_path.rglob("*.npy"):
            try:
                np.load(file_path)
            except Exception:
                corrupted_spectrograms.append(str(file_path))
        
        if corrupted_spectrograms:
            issues.append(f"–ù–∞–π–¥–µ–Ω–æ {len(corrupted_spectrograms)} –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º")
        
        return {
            'issues': issues,
            'empty_files': empty_files,
            'corrupted_audio_files': corrupted_files,
            'corrupted_spectrograms': corrupted_spectrograms,
            'quality_score': max(0, 100 - len(issues) * 10)
        }
    
    def _create_summary(self, basic_stats: Dict, audio_stats: Dict, spectrogram_stats: Dict, quality_stats: Dict) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –∞–Ω–∞–ª–∏–∑–∞"""
        return {
            'dataset_name': Path(basic_stats['dataset_path']).name,
            'total_files': basic_stats['total_files'],
            'total_size_mb': basic_stats['total_size_mb'],
            'audio_files': audio_stats.get('total_files', 0),
            'spectrogram_files': spectrogram_stats.get('total_files', 0),
            'quality_score': quality_stats['quality_score'],
            'issues_count': len(quality_stats['issues']),
            'recommendations': self._generate_recommendations(basic_stats, audio_stats, spectrogram_stats, quality_stats)
        }
    
    def _generate_recommendations(self, basic_stats: Dict, audio_stats: Dict, spectrogram_stats: Dict, quality_stats: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        recommendations = []
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É
        if basic_stats['total_size_mb'] < 100:
            recommendations.append("–î–∞—Ç–∞—Å–µ—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        if quality_stats['quality_score'] < 80:
            recommendations.append("–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ—á–∏—Å—Ç–∫–∞")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ñ–æ—Ä–º–∞—Ç—É
        if audio_stats.get('sample_rate_stats', {}).get('unique_rates'):
            unique_rates = audio_stats['sample_rate_stats']['unique_rates']
            if len(unique_rates) > 1:
                recommendations.append("–†–∞–∑–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞–º
        if spectrogram_stats.get('shape_stats', {}).get('unique_shapes'):
            unique_shapes = spectrogram_stats['shape_stats']['unique_shapes']
            if len(unique_shapes) > 1:
                recommendations.append("–†–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è")
        
        return recommendations
    
    def _save_results(self, dataset_name: str, results: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        output_dir = Path("tools/data_analysis/results")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON
        json_file = output_dir / f"{dataset_name}_analysis.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ YAML
        yaml_file = output_dir / f"{dataset_name}_analysis.yaml"
        with open(yaml_file, 'w', encoding='utf-8') as f:
            yaml.dump(results, f, default_flow_style=False, allow_unicode=True)
        
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {json_file}, {yaml_file}")
    
    def generate_report(self, dataset_name: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
        results_file = Path(f"tools/data_analysis/results/{dataset_name}_analysis.json")
        
        if not results_file.exists():
            return "–û—Ç—á–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞."
        
        with open(results_file, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        summary = results['summary']
        
        report = f"""
# –û—Ç—á–µ—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∞—Ç–∞—Å–µ—Ç–∞: {summary['dataset_name']}

## üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- **–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤**: {summary['total_files']}
- **–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞**: {summary['total_size_mb']:.2f} MB
- **–ê—É–¥–∏–æ —Ñ–∞–π–ª—ã**: {summary['audio_files']}
- **–°–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã**: {summary['spectrogram_files']}

## üîç –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
- **–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞**: {summary['quality_score']}/100
- **–ù–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã**: {summary['issues_count']}

## üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
"""
        
        for i, rec in enumerate(summary['recommendations'], 1):
            report += f"{i}. {rec}\n"
        
        return report


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    analyzer = DatasetAnalyzer()
    
    # –ê–Ω–∞–ª–∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    datasets = [
        "/home/microWakeWord_data/positives_both",
        "/home/microWakeWord_data/negatives_both",
        "/home/microWakeWord_data/generated_features"
    ]
    
    for dataset in datasets:
        if Path(dataset).exists():
            try:
                results = analyzer.analyze_dataset(dataset)
                print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è {Path(dataset).name}")
                print(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ: {results['quality_score']}/100")
                print(f"üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {len(results['recommendations'])}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {dataset}: {e}")


if __name__ == "__main__":
    main()