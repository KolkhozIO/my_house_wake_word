#!/usr/bin/env python3
"""
–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏ - —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
"""

import os
import sys
import numpy as np
import librosa
import soundfile as sf
import tensorflow as tf
from pathlib import Path
import glob
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.model_selection import train_test_split

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ microWakeWord
sys.path.append('/home/microWakeWord')
from src.utils.path_manager import paths

# –§–ò–ö–°–ò–†–£–ï–ú –°–ï–ï–î–´ –î–õ–Ø –í–û–°–ü–†–û–ò–ó–í–û–î–ò–ú–û–°–¢–ò
np.random.seed(42)
tf.random.set_seed(42)

def debug_data_distribution():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"""
    print("üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –î–ê–ù–ù–´–•:")
    print("=" * 50)
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
    positives_dir = paths.POSITIVES_RAW
    negatives_dir = paths.NEGATIVES_RAW
    
    # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–π–ª—ã
    positive_files = glob.glob(os.path.join(positives_dir, "*.wav"))
    negative_files = glob.glob(os.path.join(negatives_dir, "*.wav"))
    
    print(f"üìÅ –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(positive_files)}")
    print(f"üìÅ –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(negative_files)}")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
    print(f"\nüéµ –ê–ù–ê–õ–ò–ó –ü–û–ó–ò–¢–ò–í–ù–´–• –§–ê–ô–õ–û–í:")
    for i, file_path in enumerate(positive_files[:3]):
        try:
            audio, sr = librosa.load(file_path, sr=16000, mono=True)
            duration = len(audio) / sr
            rms = np.sqrt(np.mean(audio**2))
            print(f"  {i+1}. {os.path.basename(file_path)}: {duration:.2f}—Å, RMS: {rms:.3f}")
        except Exception as e:
            print(f"  {i+1}. {os.path.basename(file_path)}: –û–®–ò–ë–ö–ê - {e}")
    
    print(f"\nüéµ –ê–ù–ê–õ–ò–ó –ù–ï–ì–ê–¢–ò–í–ù–´–• –§–ê–ô–õ–û–í:")
    for i, file_path in enumerate(negative_files[:3]):
        try:
            audio, sr = librosa.load(file_path, sr=16000, mono=True)
            duration = len(audio) / sr
            rms = np.sqrt(np.mean(audio**2))
            print(f"  {i+1}. {os.path.basename(file_path)}: {duration:.2f}—Å, RMS: {rms:.3f}")
        except Exception as e:
            print(f"  {i+1}. {os.path.basename(file_path)}: –û–®–ò–ë–ö–ê - {e}")

def create_realistic_model():
    """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –º–æ–¥–µ–ª—å —Å –ü–†–ê–í–ò–õ–¨–ù–û–ô —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π"""
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(194, 40)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.2),  # –£–ú–ï–ù–¨–®–ï–ù–ù–ê–Ø —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dropout(0.2),  # –£–ú–ï–ù–¨–®–ï–ù–ù–ê–Ø —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dropout(0.1),  # –ú–ò–ù–ò–ú–ê–õ–¨–ù–ê–Ø —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    
    # –î–æ–±–∞–≤–ª—è–µ–º weight decay –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    optimizer = tf.keras.optimizers.Adam(
        learning_rate=0.001,
        weight_decay=1e-4  # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    )
    
    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def preprocess_audio_fixed(file_path, target_sr=16000, duration_ms=1500, seed=None):
    """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ - –ë–ï–ó —Å–ª—É—á–∞–π–Ω–æ–π –æ–±—Ä–µ–∑–∫–∏"""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
        audio, sr = librosa.load(file_path, sr=target_sr, mono=True)
        
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        target_samples = int(duration_ms * target_sr / 1000)
        if len(audio) > target_samples:
            # –§–ò–ö–°–ò–†–û–í–ê–ù–ù–ê–Ø –æ–±—Ä–µ–∑–∫–∞ - –≤—Å–µ–≥–¥–∞ —Å –Ω–∞—á–∞–ª–∞
            audio = audio[:target_samples]
        else:
            # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –µ—Å–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏–π
            audio = np.pad(audio, (0, target_samples - len(audio)))
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—É
        n_fft = 512
        hop_length = 160
        n_mels = 40
        
        # STFT
        stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)
        magnitude = np.abs(stft)
        
        # Mel-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞
        mel_spec = librosa.feature.melspectrogram(
            S=magnitude**2, 
            sr=target_sr, 
            n_mels=n_mels,
            fmax=target_sr//2
        )
        
        # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
        
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (194, 40)
        if log_mel_spec.shape[1] > 194:
            log_mel_spec = log_mel_spec[:, :194]
        else:
            # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
            pad_width = 194 - log_mel_spec.shape[1]
            log_mel_spec = np.pad(log_mel_spec, ((0, 0), (0, pad_width)))
        
        return log_mel_spec.T  # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è (194, 40)
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path}: {e}")
        return None

def test_with_realistic_data():
    """–ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ë–û–õ–¨–®–ò–ú –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö"""
    print("\nüß™ –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï:")
    print("=" * 50)
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
    positives_dir = paths.POSITIVES_RAW
    negatives_dir = paths.NEGATIVES_RAW
    
    # –£–í–ï–õ–ò–ß–ò–í–ê–ï–ú —Ä–∞–∑–º–µ—Ä –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
    positive_files = glob.glob(os.path.join(positives_dir, "*.wav"))[:200]  # 200 –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö
    negative_files = glob.glob(os.path.join(negatives_dir, "*.wav"))[:200]  # 200 –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö
    
    print(f"üìä –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ {len(positive_files)} –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –∏ {len(negative_files)} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    X = []
    y = []
    
    print("üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    for file_path in positive_files:
        spectrogram = preprocess_audio_fixed(file_path)
        if spectrogram is not None:
            X.append(spectrogram)
            y.append(1)  # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π –∫–ª–∞—Å—Å
    
    for file_path in negative_files:
        spectrogram = preprocess_audio_fixed(file_path)
        if spectrogram is not None:
            X.append(spectrogram)
            y.append(0)  # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π –∫–ª–∞—Å—Å
    
    X = np.array(X)
    y = np.array(y)
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã: {X.shape}, –º–µ—Ç–∫–∏: {y.shape}")
    print(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {np.bincount(y)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö
    pos_data = X[y == 1]
    neg_data = X[y == 0]
    
    print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ò –î–ê–ù–ù–´–•:")
    print(f"   –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ: mean={np.mean(pos_data):.3f}, std={np.std(pos_data):.3f}")
    print(f"   –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ: mean={np.mean(neg_data):.3f}, std={np.std(neg_data):.3f}")
    print(f"   –†–∞–∑–Ω–∏—Ü–∞: {abs(np.mean(pos_data) - np.mean(neg_data)):.3f}")
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test —Å –ë–û–õ–¨–®–ò–ú —Ç–µ—Å—Ç–æ–≤—ã–º –Ω–∞–±–æ—Ä–æ–º
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.4, random_state=42, stratify=y  # –£–í–ï–õ–ò–ß–ò–õ–ò test_size –¥–æ 40%
    )
    
    print(f"\nüìä Train: {X_train.shape}, Test: {X_test.shape}")
    print(f"üìä Train labels: {np.bincount(y_train)}")
    print(f"üìä Test labels: {np.bincount(y_test)}")
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = create_realistic_model()
    
    # –û–±—É—á–∞–µ–º —Å —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π –∏ —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–º learning rate
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,  # –£–í–ï–õ–ò–ß–ò–õ–ò patience
        restore_best_weights=True,
        min_delta=0.001
    )
    
    # –î–æ–±–∞–≤–ª—è–µ–º ReduceLROnPlateau
    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-6
    )
    
    print(f"\nüèãÔ∏è –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    history = model.fit(
        X_train, y_train,
        epochs=100,  # –£–í–ï–õ–ò–ß–ò–õ–ò –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        batch_size=32,  # –£–í–ï–õ–ò–ß–ò–õ–ò batch size
        validation_data=(X_test, y_test),
        callbacks=[early_stopping, reduce_lr],
        verbose=1
    )
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred_proba = model.predict(X_test)
    y_pred = (y_pred_proba > 0.5).astype(int).flatten()
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    
    try:
        auc = roc_auc_score(y_test, y_pred_proba)
    except:
        auc = 0.0
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    # Wake Word —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    frr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Reject Rate
    far = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Accept Rate
    
    print(f"\nüìä –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï –ú–ï–¢–†–ò–ö–ò:")
    print(f"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   Precision: {precision:.4f} ({precision*100:.2f}%)")
    print(f"   Recall:    {recall:.4f} ({recall*100:.2f}%)")
    print(f"   F1-Score:  {f1:.4f} ({f1*100:.2f}%)")
    print(f"   AUC:       {auc:.4f}")
    print(f"   FRR:       {frr:.4f} ({frr*100:.2f}%)")
    print(f"   FAR:       {far:.4f} ({far*100:.2f}%)")
    
    print(f"\nüìä CONFUSION MATRIX:")
    print(f"   True Negatives:  {tn:3d}")
    print(f"   False Positives: {fp:3d}")
    print(f"   False Negatives: {fn:3d}")
    print(f"   True Positives:  {tp:3d}")
    
    # –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    print(f"\nüéØ –ê–ù–ê–õ–ò–ó –£–í–ï–†–ï–ù–ù–û–°–¢–ò:")
    pos_confidences = y_pred_proba[y_test == 1]
    neg_confidences = y_pred_proba[y_test == 0]
    
    if len(pos_confidences) > 0:
        print(f"   –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ: –º–∏–Ω={np.min(pos_confidences):.3f}, –º–∞–∫—Å={np.max(pos_confidences):.3f}, —Å—Ä–µ–¥–Ω–µ–µ={np.mean(pos_confidences):.3f}")
    if len(neg_confidences) > 0:
        print(f"   –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ: –º–∏–Ω={np.min(neg_confidences):.3f}, –º–∞–∫—Å={np.max(neg_confidences):.3f}, —Å—Ä–µ–¥–Ω–µ–µ={np.mean(neg_confidences):.3f}")
    
    # –ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
    print(f"\nüìà –ê–ù–ê–õ–ò–ó –û–ë–£–ß–ï–ù–ò–Ø:")
    final_train_acc = history.history['accuracy'][-1]
    final_val_acc = history.history['val_accuracy'][-1]
    final_train_loss = history.history['loss'][-1]
    final_val_loss = history.history['val_loss'][-1]
    
    print(f"   Final Train Accuracy: {final_train_acc:.4f}")
    print(f"   Final Val Accuracy:   {final_val_acc:.4f}")
    print(f"   Final Train Loss:     {final_val_loss:.4f}")
    print(f"   Final Val Loss:       {final_val_loss:.4f}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
    if final_train_acc - final_val_acc > 0.1:
        print("‚ö†Ô∏è –ü–†–ò–ó–ù–ê–ö–ò –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø: Train accuracy –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –≤—ã—à–µ Val accuracy")
    elif final_val_acc > 0.95:
        print("‚ö†Ô∏è –ü–†–ò–ó–ù–ê–ö–ò –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø: –°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∞—è Val accuracy")
    else:
        print("‚úÖ –ú–æ–¥–µ–ª—å –≤—ã–≥–ª—è–¥–∏—Ç –∑–¥–æ—Ä–æ–≤–æ–π")
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc': auc,
        'frr': frr,
        'far': far,
        'confusion_matrix': cm,
        'history': history
    }

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ú–û–î–ï–õ–ò")
    print("=" * 60)
    
    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    debug_data_distribution()
    
    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    metrics = test_with_realistic_data()
    
    print(f"\nüéâ –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if metrics['accuracy'] > 0.95:
        print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å - –≤–æ–∑–º–æ–∂–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ!")
    elif metrics['accuracy'] > 0.85:
        print("‚úÖ –•–æ—Ä–æ—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å - –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    else:
        print("‚ùå –ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å - –º–æ–¥–µ–ª—å –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ —É–ª—É—á—à–µ–Ω–∏–∏")

if __name__ == "__main__":
    main()