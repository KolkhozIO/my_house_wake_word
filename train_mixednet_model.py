#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ MixedNet –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import tensorflow as tf
import numpy as np
import os
import librosa
from pathlib import Path

def load_audio_file(file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—É–¥–∏–æ —Ñ–∞–π–ª"""
    try:
        audio, sr = librosa.load(file_path, sr=16000)
        return audio
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
        return None

def create_mel_spectrogram(audio):
    """–°–æ–∑–¥–∞–µ—Ç mel-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—É"""
    try:
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª–∏–Ω—ã –¥–æ 1.5 —Å–µ–∫—É–Ω–¥ (24000 —Å—ç–º–ø–ª–æ–≤ –ø—Ä–∏ 16kHz)
        target_length = 24000
        if len(audio) > target_length:
            audio = audio[:target_length]
        else:
            audio = np.pad(audio, (0, target_length - len(audio)))
        
        # –°–æ–∑–¥–∞–Ω–∏–µ mel-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=16000,
            n_mels=40,
            hop_length=160,
            n_fft=512
        )
        
        # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
        
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (40 x 49)
        if mel_spec.shape[1] > 49:
            mel_spec = mel_spec[:, :49]
        elif mel_spec.shape[1] < 49:
            mel_spec = np.pad(mel_spec, ((0, 0), (0, 49 - mel_spec.shape[1])))
        
        return mel_spec
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã: {e}")
        return None

def load_real_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
    features = []
    labels = []
    
    # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Å—ç–º–ø–ª—ã
    pos_dir = Path("piper-sample-generator/positives_combined_aug")
    if pos_dir.exists():
        pos_files = list(pos_dir.glob("*.wav"))
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(pos_files)} –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
        
        for i, file_path in enumerate(pos_files[:100]):  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 100
            if i % 10 == 0:
                print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ: {i}/{min(100, len(pos_files))}")
            
            audio = load_audio_file(file_path)
            if audio is not None:
                mel_spec = create_mel_spectrogram(audio)
                if mel_spec is not None:
                    features.append(mel_spec)
                    labels.append(1)  # –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å
    
    # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Å—ç–º–ø–ª—ã
    neg_dir = Path("piper-sample-generator/negatives_moy_dom_massive_aug")
    if neg_dir.exists():
        neg_files = list(neg_dir.glob("*.wav"))
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(neg_files)} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
        
        for i, file_path in enumerate(neg_files[:50]):  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 50
            if i % 10 == 0:
                print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ: {i}/{min(50, len(neg_files))}")
            
            audio = load_audio_file(file_path)
            if audio is not None:
                mel_spec = create_mel_spectrogram(audio)
                if mel_spec is not None:
                    features.append(mel_spec)
                    labels.append(0)  # –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –∫–ª–∞—Å—Å
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(features)} —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º")
    print(f"   –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö: {sum(labels)}")
    print(f"   –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {len(labels) - sum(labels)}")
    
    return np.array(features), np.array(labels)

def create_mixednet_model():
    """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å MixedNet –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ"""
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    pointwise_filters = [48, 48, 48, 48]
    repeat_in_block = [1, 1, 1, 1]
    mixconv_kernel_sizes = [[5], [9], [13], [21]]
    residual_connections = [0, 0, 0, 0, 0]
    
    input_audio = tf.keras.layers.Input(shape=(40, 49))
    net = input_audio
    
    # make it [batch, time, 1, feature]
    net = tf.keras.layers.Reshape((40, 49, 1))(net)
    
    # –ü–µ—Ä–≤—ã–π Conv2D —Å–ª–æ–π —Å padding="same" —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ä
    net = tf.keras.layers.Conv2D(
        32,
        (3, 1),
        strides=(1, 1),
        padding="same",
        use_bias=False,
    )(net)
    net = tf.keras.layers.Activation("relu")(net)
    
    # Encoder –±–ª–æ–∫–∏ —Å padding="same"
    for filters, repeat, ksize in zip(pointwise_filters, repeat_in_block, mixconv_kernel_sizes):
        for _ in range(repeat):
            if max(ksize) > 1:
                # DepthwiseConv2D –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å padding="same"
                net = tf.keras.layers.DepthwiseConv2D(
                    (ksize[0], 1), strides=1, padding="same"
                )(net)
            
            # Pointwise convolution
            net = tf.keras.layers.Conv2D(
                filters=filters, kernel_size=1, use_bias=False, padding="same"
            )(net)
            net = tf.keras.layers.BatchNormalization()(net)
            net = tf.keras.layers.Activation("relu")(net)
    
    # Global Average Pooling
    net = tf.keras.layers.GlobalAveragePooling2D()(net)
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–π Dense —Å–ª–æ–π
    net = tf.keras.layers.Dense(1, activation="sigmoid")(net)
    
    model = tf.keras.Model(input_audio, net)
    
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üì¶ –ó–∞–≥—Ä—É–∂–∞—é —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...")
    features, labels = load_real_data()
    
    if len(features) == 0:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return
    
    print("üèóÔ∏è –°–æ–∑–¥–∞—é –º–æ–¥–µ–ª—å MixedNet...")
    model = create_mixednet_model()
    
    print("üéØ –û–±—É—á–∞—é –º–æ–¥–µ–ª—å...")
    model.fit(features, labels, epochs=10, validation_split=0.2, verbose=1)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_dir = "models/trained_model"
    os.makedirs(model_dir, exist_ok=True)
    
    # H5 –º–æ–¥–µ–ª—å
    model_path = os.path.join(model_dir, "wake_word_model.h5")
    model.save(model_path)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ TFLite
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    tflite_model = converter.convert()
    
    tflite_path = os.path.join(model_dir, "wake_word_model.tflite")
    with open(tflite_path, 'wb') as f:
        f.write(tflite_model)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
    size_kb = os.path.getsize(tflite_path) / 1024
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å MixedNet –æ–±—É—á–µ–Ω–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
    print(f"üìä –†–∞–∑–º–µ—Ä: {size_kb:.1f} KB")
    print(f"üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {tflite_path}")

if __name__ == "__main__":
    main()