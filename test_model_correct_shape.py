#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ñ–æ—Ä–º–æ–π –¥–∞–Ω–Ω—ã—Ö (147, 40)
"""

import sys
import os
sys.path.append('/home/microWakeWord')

import numpy as np
import tensorflow as tf
from pathlib import Path
import random

def load_spectrograms_correct_shape(data_dir, max_samples=50):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã (147, 40)"""
    
    print(f"üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {data_dir}")
    
    try:
        # –ò—â–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ wakeword_mmap
        mmap_dirs = list(Path(data_dir).glob("*/wakeword_mmap"))
        if not mmap_dirs:
            print(f"‚ùå RaggedMmap –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {data_dir}")
            return None
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        mmap_dir = mmap_dirs[0]
        print(f"üìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º: {mmap_dir}")
        
        from microwakeword.data import RaggedMmap
        data = RaggedMmap(mmap_dir)
        print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(data)}")
        
        # –ë–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã
        indices = random.sample(range(len(data)), min(max_samples, len(data)))
        spectrograms = []
        
        for i, idx in enumerate(indices):
            if i % 20 == 0:
                print(f"üîÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {i}/{len(indices)}")
            
            spec = data[idx]
            if spec.shape == (147, 40):  # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –¥–ª—è –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
                spectrograms.append(spec)
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º: {len(spectrograms)}")
        return np.array(spectrograms)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return None

def test_model_with_padding(model_path, spectrograms, label="Unknown"):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö –¥–æ –Ω—É–∂–Ω–æ–π —Ñ–æ—Ä–º—ã"""
    
    print(f"üîÑ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ {label} –¥–∞–Ω–Ω—ã—Ö...")
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º TFLite –º–æ–¥–µ–ª—å
        interpreter = tf.lite.Interpreter(model_path=model_path)
        interpreter.allocate_tensors()
        
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        expected_shape = input_details[0]['shape'][1:]  # (194, 40)
        print(f"üìä –û–∂–∏–¥–∞–µ–º–∞—è —Ñ–æ—Ä–º–∞: {expected_shape}")
        print(f"üìä –§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {spectrograms.shape}")
        
        # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–æ –Ω—É–∂–Ω–æ–π —Ñ–æ—Ä–º—ã
        padded_spectrograms = []
        for spec in spectrograms:
            if spec.shape[0] < expected_shape[0]:
                # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                padding = np.zeros((expected_shape[0] - spec.shape[0], spec.shape[1]))
                padded_spec = np.vstack([spec, padding])
            elif spec.shape[0] > expected_shape[0]:
                # –û–±—Ä–µ–∑–∞–µ–º
                padded_spec = spec[:expected_shape[0]]
            else:
                padded_spec = spec
            
            padded_spectrograms.append(padded_spec)
        
        padded_spectrograms = np.array(padded_spectrograms)
        print(f"üìä –§–æ—Ä–º–∞ –ø–æ—Å–ª–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è: {padded_spectrograms.shape}")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–∞—Ç—á–∞
        batch_size = input_details[0]['shape'][0]
        results = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ –±–∞—Ç—á–∞–º
        for i in range(0, len(padded_spectrograms), batch_size):
            batch = padded_spectrograms[i:i+batch_size]
            
            # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–æ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if len(batch) < batch_size:
                # –î—É–±–ª–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç
                while len(batch) < batch_size:
                    batch = np.vstack([batch, batch[-1:]])
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
            interpreter.set_tensor(input_details[0]['index'], batch.astype(np.float32))
            interpreter.invoke()
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            output = interpreter.get_tensor(output_details[0]['index'])
            results.extend(output[:len(padded_spectrograms[i:i+batch_size])])
        
        results = np.array(results)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è {label}:")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ: {np.mean(results):.4f}")
        print(f"  –ú–µ–¥–∏–∞–Ω–∞: {np.median(results):.4f}")
        print(f"  –ú–∏–Ω: {np.min(results):.4f}")
        print(f"  –ú–∞–∫—Å: {np.max(results):.4f}")
        print(f"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.std(results):.4f}")
        
        # –°—á–∏—Ç–∞–µ–º –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ > 0.5)
        activations = np.sum(results > 0.5)
        activation_rate = activations / len(results)
        print(f"  –ê–∫—Ç–∏–≤–∞—Ü–∏–∏ (>0.5): {activations}/{len(results)} ({activation_rate:.2%})")
        
        return results
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    print("üéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ñ–æ—Ä–º–æ–π –¥–∞–Ω–Ω—ã—Ö")
    print("=" * 60)
    
    model_path = "/home/microWakeWord_data/trained_models/wakeword/model.tflite"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    if not os.path.exists(model_path):
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
        return False
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\nüéØ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –ü–û–ó–ò–¢–ò–í–ù–´–• –î–ê–ù–ù–´–•")
    print("-" * 40)
    
    positives_dir = "/home/microWakeWord_data/features_positives"
    positive_data = load_spectrograms_correct_shape(positives_dir, max_samples=30)
    
    if positive_data is not None:
        positive_results = test_model_with_padding(model_path, positive_data, "–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ")
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        positive_results = None
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\nüéØ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –ù–ï–ì–ê–¢–ò–í–ù–´–• –î–ê–ù–ù–´–•")
    print("-" * 40)
    
    negatives_dir = "/home/microWakeWord_data/features_negatives"
    negative_data = load_spectrograms_correct_shape(negatives_dir, max_samples=30)
    
    if negative_data is not None:
        negative_results = test_model_with_padding(model_path, negative_data, "–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ")
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        negative_results = None
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüìä –û–ë–©–ò–ô –ê–ù–ê–õ–ò–ó")
    print("=" * 40)
    
    if positive_results is not None and negative_results is not None:
        # –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        pos_mean = np.mean(positive_results)
        neg_mean = np.mean(negative_results)
        
        pos_activations = np.sum(positive_results > 0.5) / len(positive_results)
        neg_activations = np.sum(negative_results > 0.5) / len(negative_results)
        
        print(f"üìà –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
        print(f"  –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {pos_mean:.4f}")
        print(f"  –ê–∫—Ç–∏–≤–∞—Ü–∏–∏: {pos_activations:.2%}")
        
        print(f"üìâ –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
        print(f"  –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {neg_mean:.4f}")
        print(f"  –ê–∫—Ç–∏–≤–∞—Ü–∏–∏: {neg_activations:.2%}")
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
        separation = pos_mean - neg_mean
        print(f"üéØ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {separation:.4f}")
        
        if separation > 0.1:
            print("‚úÖ –•–æ—Ä–æ—à–µ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤!")
        elif separation > 0.05:
            print("‚ö†Ô∏è –£–º–µ—Ä–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤")
        else:
            print("‚ùå –ü–ª–æ—Ö–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤")
        
        # Wake word –º–µ—Ç—Ä–∏–∫–∏
        print(f"\nüé§ WAKE WORD –ú–ï–¢–†–ò–ö–ò:")
        print(f"  FRR (False Reject Rate): {1 - pos_activations:.2%}")
        print(f"  FAR (False Accept Rate): {neg_activations:.2%}")
        
        if pos_activations > 0.8 and neg_activations < 0.1:
            print("üéâ –û–¢–õ–ò–ß–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´!")
        elif pos_activations > 0.7 and neg_activations < 0.2:
            print("‚úÖ –•–û–†–û–®–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´!")
        else:
            print("‚ö†Ô∏è –¢–†–ï–ë–£–ï–¢–°–Ø –î–û–†–ê–ë–û–¢–ö–ê")
    
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return False
    
    print("\nüéâ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    return True

if __name__ == "__main__":
    success = main()
    if not success:
        sys.exit(1)