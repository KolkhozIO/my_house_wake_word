#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ microWakeWord –Ω–∞ —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
"""

import os
import sys
import numpy as np
import tensorflow as tf
from pathlib import Path
import json
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ
sys.path.append(os.path.join(os.path.dirname(__file__), 'microwakeword'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.utils.path_manager import paths

def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
    print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    
    # –ü—É—Ç—å –∫ –≤–µ—Å–∞–º –º–æ–¥–µ–ª–∏
    weights_path = "/home/microWakeWord_data/trained_models/wakeword/best_weights.weights.h5"
    
    if not os.path.exists(weights_path):
        print(f"‚ùå –í–µ—Å–∞ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {weights_path}")
        return None
    
    try:
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã microWakeWord
        import microwakeword.mixednet as mixednet
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç flags –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        class Flags:
            def __init__(self):
                self.pointwise_filters = "48,48,48,48"
                self.repeat_in_block = "1,1,1,1"
                self.mixconv_kernel_sizes = "[5],[9],[13],[21]"
                self.residual_connection = "0,0,0,0"
                self.first_conv_filters = 32
                self.first_conv_kernel_size = 3
                self.stride = 1
                self.spatial_attention = False
                self.temporal_attention = False
                self.attention_heads = 1
                self.attention_dim = 64
                self.pooled = False
                self.max_pool = False
        
        flags = Flags()
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        model = mixednet.model(flags, (194, 40), 32)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
        model.load_weights(weights_path)
        
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        return model
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        import traceback
        traceback.print_exc()
        return None

def load_test_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ"""
    print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    try:
        import microwakeword.data as input_data
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö
        data_processor = input_data.FeatureHandler({
            'features': [
                {
                    'features_dir': paths.FEATURES_POSITIVES,
                    'penalty_weight': 1.0,
                    'sampling_weight': 1.0,
                    'truncation_strategy': 'random',
                    'truth': True,
                    'type': 'mmap'
                },
                {
                    'features_dir': paths.FEATURES_NEGATIVES,
                    'penalty_weight': 1.0,
                    'sampling_weight': 1.0,
                    'truncation_strategy': 'random',
                    'truth': False,
                    'type': 'mmap'
                }
            ],
            'batch_size': 32,
            'training_input_shape': (194, 40),
            'stride': 1,
            'window_step_ms': 10
        })
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ–º train —Ä–µ–∂–∏–º)
        test_data = data_processor.get_data(mode="train", batch_size=32, features_length=194)
        
        print(f"‚úÖ –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        return test_data
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        import traceback
        traceback.print_exc()
        return None

def evaluate_model(model, test_data):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print("üîÑ –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...")
    
    try:
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
        predictions = []
        true_labels = []
        
        for batch_x, batch_y in test_data:
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            batch_pred = model.predict(batch_x, verbose=0)
            predictions.extend(batch_pred.flatten())
            true_labels.extend(batch_y.flatten())
        
        predictions = np.array(predictions)
        true_labels = np.array(true_labels)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
        
        # –ë–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–ø–æ—Ä–æ–≥ 0.5)
        binary_pred = (predictions > 0.5).astype(int)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(true_labels, binary_pred)
        precision = precision_score(true_labels, binary_pred, zero_division=0)
        recall = recall_score(true_labels, binary_pred, zero_division=0)
        f1 = f1_score(true_labels, binary_pred, zero_division=0)
        
        # AUC (–µ—Å–ª–∏ –µ—Å—Ç—å –æ–±–∞ –∫–ª–∞—Å—Å–∞)
        try:
            auc = roc_auc_score(true_labels, predictions)
        except ValueError:
            auc = 0.0
        
        # Confusion Matrix
        cm = confusion_matrix(true_labels, binary_pred)
        
        # Wake Word —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        tn, fp, fn, tp = cm.ravel()
        
        # False Reject Rate (FRR) - –ø—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö wake words
        frr = fn / (fn + tp) if (fn + tp) > 0 else 0
        
        # False Accept Rate (FAR) - –ø—Ä–æ—Ü–µ–Ω—Ç –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
        far = fp / (fp + tn) if (fp + tn) > 0 else 0
        
        # False Accepts per Hour (FA/h) - –ø—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç 1 —á–∞—Å = 3600 —Å–µ–∫—É–Ω–¥
        # –ö–∞–∂–¥—ã–µ 10ms = 100 –ø—Ä–æ–≤–µ—Ä–æ–∫ –≤ —Å–µ–∫—É–Ω–¥—É = 360,000 –ø—Ä–æ–≤–µ—Ä–æ–∫ –≤ —á–∞—Å
        checks_per_hour = 360000
        fa_per_hour = far * checks_per_hour
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'frr': frr,
            'far': far,
            'fa_per_hour': fa_per_hour,
            'confusion_matrix': cm,
            'total_samples': len(true_labels),
            'positive_samples': int(np.sum(true_labels)),
            'negative_samples': int(len(true_labels) - np.sum(true_labels))
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        import traceback
        traceback.print_exc()
        return None

def print_metrics(metrics):
    """–í—ã–≤–æ–¥–∏—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤ –∫—Ä–∞—Å–∏–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
    print("\n" + "="*60)
    print("üìä –ú–ï–¢–†–ò–ö–ò –û–ë–£–ß–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò microWakeWord")
    print("="*60)
    
    print(f"\nüìà –û–ë–©–ò–ï –ú–ï–¢–†–ò–ö–ò:")
    print(f"   Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
    print(f"   Precision: {metrics['precision']:.4f} ({metrics['precision']*100:.2f}%)")
    print(f"   Recall:    {metrics['recall']:.4f} ({metrics['recall']*100:.2f}%)")
    print(f"   F1-Score:  {metrics['f1']:.4f} ({metrics['f1']*100:.2f}%)")
    print(f"   AUC:       {metrics['auc']:.4f}")
    
    print(f"\nüéØ WAKE WORD –°–ü–ï–¶–ò–§–ò–ß–ù–´–ï –ú–ï–¢–†–ò–ö–ò:")
    print(f"   FRR (False Reject Rate):     {metrics['frr']:.4f} ({metrics['frr']*100:.2f}%)")
    print(f"   FAR (False Accept Rate):     {metrics['far']:.4f} ({metrics['far']*100:.2f}%)")
    print(f"   FA/h (False Accepts/hour):  {metrics['fa_per_hour']:.1f}")
    
    print(f"\nüìä –î–ê–ù–ù–´–ï:")
    print(f"   –í—Å–µ–≥–æ –æ–±—Ä–∞–∑—Ü–æ–≤:     {metrics['total_samples']}")
    print(f"   –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö:        {metrics['positive_samples']}")
    print(f"   –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö:        {metrics['negative_samples']}")
    print(f"   –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ:       {metrics['negative_samples']/metrics['positive_samples']:.1f}:1")
    
    print(f"\nüî¢ CONFUSION MATRIX:")
    cm = metrics['confusion_matrix']
    print(f"   True Negatives:  {cm[0,0]:6d}")
    print(f"   False Positives: {cm[0,1]:6d}")
    print(f"   False Negatives: {cm[1,0]:6d}")
    print(f"   True Positives:  {cm[1,1]:6d}")
    
    print(f"\n‚úÖ –û–¶–ï–ù–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
    
    # –û—Ü–µ–Ω–∫–∞ –ø–æ wake word —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º
    if metrics['frr'] < 0.05:
        print(f"   ‚úÖ FRR –æ—Ç–ª–∏—á–Ω—ã–π (< 5%)")
    elif metrics['frr'] < 0.1:
        print(f"   ‚ö†Ô∏è  FRR —Ö–æ—Ä–æ—à–∏–π (< 10%)")
    else:
        print(f"   ‚ùå FRR –ø–ª–æ—Ö–æ–π (> 10%)")
    
    if metrics['fa_per_hour'] < 1.0:
        print(f"   ‚úÖ FA/h –æ—Ç–ª–∏—á–Ω—ã–π (< 1/—á–∞—Å)")
    elif metrics['fa_per_hour'] < 5.0:
        print(f"   ‚ö†Ô∏è  FA/h —Ö–æ—Ä–æ—à–∏–π (< 5/—á–∞—Å)")
    else:
        print(f"   ‚ùå FA/h –ø–ª–æ—Ö–æ–π (> 5/—á–∞—Å)")
    
    if metrics['auc'] > 0.9:
        print(f"   ‚úÖ AUC –æ—Ç–ª–∏—á–Ω—ã–π (> 0.9)")
    elif metrics['auc'] > 0.8:
        print(f"   ‚ö†Ô∏è  AUC —Ö–æ—Ä–æ—à–∏–π (> 0.8)")
    else:
        print(f"   ‚ùå AUC –ø–ª–æ—Ö–æ–π (< 0.8)")
    
    print("="*60)

def save_metrics(metrics):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–∞–π–ª"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    metrics_file = f"/home/microWakeWord_data/model_metrics_{timestamp}.json"
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è JSON
    metrics_json = {
        'timestamp': timestamp,
        'model_path': '/home/microWakeWord_data/trained_models/wakeword/best_weights.weights.h5',
        'metrics': {
            'accuracy': float(metrics['accuracy']),
            'precision': float(metrics['precision']),
            'recall': float(metrics['recall']),
            'f1': float(metrics['f1']),
            'auc': float(metrics['auc']),
            'frr': float(metrics['frr']),
            'far': float(metrics['far']),
            'fa_per_hour': float(metrics['fa_per_hour']),
            'total_samples': int(metrics['total_samples']),
            'positive_samples': int(metrics['positive_samples']),
            'negative_samples': int(metrics['negative_samples'])
        },
        'confusion_matrix': metrics['confusion_matrix'].tolist()
    }
    
    with open(metrics_file, 'w') as f:
        json.dump(metrics_json, f, indent=2)
    
    print(f"üíæ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metrics_file}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ microWakeWord")
    print("="*50)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model = load_model()
    if model is None:
        return False
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_data = load_test_data()
    if test_data is None:
        return False
    
    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
    metrics = evaluate_model(model, test_data)
    if metrics is None:
        return False
    
    # –í—ã–≤–æ–¥–∏–º –º–µ—Ç—Ä–∏–∫–∏
    print_metrics(metrics)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    save_metrics(metrics)
    
    print("\nüéâ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
    return True

if __name__ == "__main__":
    main()