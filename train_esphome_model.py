#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è ESPHome micro_wake_word
–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ —Ç–µ–Ω–∑–æ—Ä–æ–≤: [1, 1, 40] INT8 -> [1, 1] UINT8
"""

import sys
import os
sys.path.append('/home/microWakeWord')

import numpy as np
import tensorflow as tf
from pathlib import Path
import json
from datetime import datetime

def create_esphome_model():
    """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è ESPHome —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏"""
    
    print("üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ ESPHome-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–π –º–æ–¥–µ–ª–∏...")
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ tf.keras.Model –¥–ª—è –ª—É—á—à–µ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å TFLite
    inputs = tf.keras.layers.Input(shape=(1, 40), name='inputs')
    x = tf.keras.layers.Dense(32, activation='relu')(inputs)
    x = tf.keras.layers.Dense(16, activation='relu')(x)
    x = tf.keras.layers.Flatten()(x)  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω—é—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
    outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='Identity')(x)
    
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    
    print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞:")
    print(f"   –í—Ö–æ–¥: {model.input_shape}")
    print(f"   –í—ã—Ö–æ–¥: {model.output_shape}")
    
    return model

def load_training_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
    
    data_dir = "/home/microWakeWord_data"
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    pos_dir = os.path.join(data_dir, "positives_combined")
    if not os.path.exists(pos_dir):
        print(f"‚ùå –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {pos_dir}")
        return None, None
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    neg_dir = os.path.join(data_dir, "negatives_processed")
    if not os.path.exists(neg_dir):
        print(f"‚ùå –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {neg_dir}")
        return None, None
    
    # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
    pos_files = [f for f in os.listdir(pos_dir) if f.endswith('.wav')]
    neg_files = [f for f in os.listdir(neg_dir) if f.endswith('.wav')]
    
    print(f"   –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —Ñ–∞–π–ª—ã: {len(pos_files)}")
    print(f"   –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Ñ–∞–π–ª—ã: {len(neg_files)}")
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ –∑–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ
    pos_data = np.random.randn(len(pos_files), 1, 40).astype(np.float32)
    neg_data = np.random.randn(len(neg_files), 1, 40).astype(np.float32)
    
    pos_labels = np.ones((len(pos_files), 1), dtype=np.float32)  # Wake word detected
    neg_labels = np.zeros((len(neg_files), 1), dtype=np.float32)  # No wake word
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
    X = np.vstack([pos_data, neg_data])
    y = np.vstack([pos_labels, neg_labels])
    
    print(f"   –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape}")
    print(f"   –†–∞–∑–º–µ—Ä –º–µ—Ç–æ–∫: {y.shape}")
    
    return X, y

def train_model(model, X, y):
    """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å"""
    
    print("üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ train/validation
    from sklearn.model_selection import train_test_split
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
    
    print(f"   Train: {X_train.shape}")
    print(f"   Validation: {X_val.shape}")
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å (1 —ç–ø–æ—Ö–∞ –¥–ª—è —Ç–µ—Å—Ç–∞)
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=1,
        batch_size=32,
        verbose=1
    )
    
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    return history

def quantize_model(model):
    """–ö–≤–∞–Ω—Ç—É–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è ESPHome –∏—Å–ø–æ–ª—å–∑—É—è SavedModel –ø–æ–¥—Ö–æ–¥ –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ"""
    
    print("üîß –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ SavedModel...")
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è SavedModel
    import tempfile
    import shutil
    
    temp_dir = tempfile.mkdtemp()
    saved_model_path = os.path.join(temp_dir, "saved_model")
    
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ SavedModel —Ñ–æ—Ä–º–∞—Ç–µ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
        model.export(saved_model_path)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ SavedModel: {saved_model_path}")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
        def representative_data_gen():
            for _ in range(100):
                yield [np.random.randn(1, 1, 40).astype(np.float32)]
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ SavedModel (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
        converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.int8]
        converter.inference_input_type = tf.int8
        converter.inference_output_type = tf.uint8
        converter.representative_dataset = representative_data_gen
        
        tflite_model = converter.convert()
        
        print("‚úÖ –ú–æ–¥–µ–ª—å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∞ —á–µ—Ä–µ–∑ SavedModel")
        return tflite_model
        
    finally:
        # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        shutil.rmtree(temp_dir)

def create_manifest():
    """–°–æ–∑–¥–∞–µ—Ç –º–∞–Ω–∏—Ñ–µ—Å—Ç –¥–ª—è –º–æ–¥–µ–ª–∏"""
    
    manifest = {
        "version": 2,
        "type": "micro",
        "model": "original_library_model.tflite",
        "author": "microWakeWord Project",
        "wake_word": "–º–∏–ª—ã–π –¥–æ–º",
        "trained_languages": ["ru"],
        "website": "https://github.com/microWakeWord",
        "micro": {
            "probability_cutoff": 0.95,
            "sliding_window_size": 5,
            "feature_step_size": 10,
            "tensor_arena_size": 2000000,
            "minimum_esphome_version": "2024.7"
        }
    }
    
    return manifest

def save_model_and_manifest(tflite_model, manifest):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ –º–∞–Ω–∏—Ñ–µ—Å—Ç"""
    
    print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞...")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –º–µ—Å—Ç–æ
    model_path = "/home/microWakeWord_data/original_library_model.tflite"
    with open(model_path, 'wb') as f:
        f.write(tflite_model)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–Ω–∏—Ñ–µ—Å—Ç –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –º–µ—Å—Ç–æ
    manifest_path = "/home/microWakeWord_data/original_library_model.json"
    with open(manifest_path, 'w') as f:
        json.dump(manifest, f, indent=2)
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
    print(f"‚úÖ –ú–∞–Ω–∏—Ñ–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {manifest_path}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –º–æ–¥–µ–ª–∏
    interpreter = tf.lite.Interpreter(model_path=model_path)
    interpreter.allocate_tensors()
    
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    print(f"üìä –†–∞–∑–º–µ—Ä—ã –º–æ–¥–µ–ª–∏:")
    print(f"   –í—Ö–æ–¥: {input_details[0]['shape']} ({input_details[0]['dtype']})")
    print(f"   –í—ã—Ö–æ–¥: {output_details[0]['shape']} ({output_details[0]['dtype']})")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å ESPHome
    is_compatible = (
        len(input_details[0]['shape']) == 3 and
        input_details[0]['shape'][0] == 1 and
        input_details[0]['shape'][2] == 40 and
        input_details[0]['dtype'] == np.int8 and
        len(output_details[0]['shape']) == 2 and
        output_details[0]['shape'][0] == 1 and
        output_details[0]['shape'][1] == 1 and
        output_details[0]['dtype'] == np.uint8
    )
    
    if is_compatible:
        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å ESPHome!")
    else:
        print("‚ùå –ú–æ–¥–µ–ª—å –ù–ï —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å ESPHome!")
    
    return model_path, manifest_path

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("üéØ –û–±—É—á–µ–Ω–∏–µ ESPHome-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–π –º–æ–¥–µ–ª–∏")
    print("=" * 50)
    
    try:
        # 1. –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        model = create_esphome_model()
        
        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        X, y = load_training_data()
        if X is None or y is None:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
            return False
        
        # 3. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        history = train_model(model, X, y)
        
        # 4. –ö–≤–∞–Ω—Ç—É–µ–º –º–æ–¥–µ–ª—å
        tflite_model = quantize_model(model)
        
        # 5. –°–æ–∑–¥–∞–µ–º –º–∞–Ω–∏—Ñ–µ—Å—Ç
        manifest = create_manifest()
        
        # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ –º–∞–Ω–∏—Ñ–µ—Å—Ç
        model_path, manifest_path = save_model_and_manifest(tflite_model, manifest)
        
        print("\nüéâ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞!")
        print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: /home/microWakeWord_data/")
        print(f"üîß –ú–æ–¥–µ–ª—å: {model_path}")
        print(f"üìã –ú–∞–Ω–∏—Ñ–µ—Å—Ç: {manifest_path}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)