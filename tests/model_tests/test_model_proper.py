#!/usr/bin/env python3
"""
–ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
"""

import os
import sys
import numpy as np
import librosa
import soundfile as sf
import tensorflow as tf
from pathlib import Path
import glob

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ microWakeWord
sys.path.append('/home/microWakeWord')
from src.utils.path_manager import paths
from microwakeword import mixednet
from microwakeword.layers import modes

def create_model():
    """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π microWakeWord"""
    print("üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π...")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏–∑ training_config.yaml
    flags = {
        'attention_dim': 64,
        'attention_heads': 1,
        'first_conv_filters': 32,
        'first_conv_kernel_size': 3,
        'max_pool': False,
        'mixconv_kernel_sizes': '[5],[9],[13],[21]',
        'pointwise_filters': '48,48,48,48',
        'pooled': False,
        'repeat_in_block': '1,1,1,1',
        'residual_connection': '0,0,0,0',
        'spatial_attention': False,
        'stride': 1,
        'temporal_attention': False
    }
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    model = mixednet.model(
        flags=flags,
        shape=(194, 40),
        batch_size=1
    )
    
    print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
    return model

def load_trained_weights(model):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞"""
    weights_path = "/home/microWakeWord_data/trained_models/wakeword_mixed_–∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π/best_weights.weights.h5"
    
    if os.path.exists(weights_path):
        try:
            model.load_weights(weights_path)
            print("‚úÖ –í–µ—Å–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤: {e}")
            return False
    else:
        print("‚ùå –§–∞–π–ª –≤–µ—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return False

def preprocess_audio_proper(file_path, target_sr=16000, duration_ms=1500):
    """–ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ microWakeWord"""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
        audio, sr = librosa.load(file_path, sr=target_sr, mono=True)
        
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        target_samples = int(duration_ms * target_sr / 1000)
        if len(audio) > target_samples:
            audio = audio[:target_samples]
        else:
            # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –µ—Å–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏–π
            audio = np.pad(audio, (0, target_samples - len(audio)))
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—É –∫–∞–∫ –≤ microWakeWord
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        n_fft = 512
        hop_length = 160
        n_mels = 40
        
        # STFT
        stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)
        magnitude = np.abs(stft)
        
        # Mel-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞
        mel_spec = librosa.feature.melspectrogram(
            S=magnitude**2, 
            sr=target_sr, 
            n_mels=n_mels,
            fmax=target_sr//2
        )
        
        # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
        
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (194, 40)
        if log_mel_spec.shape[1] > 194:
            log_mel_spec = log_mel_spec[:, :194]
        else:
            # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
            pad_width = 194 - log_mel_spec.shape[1]
            log_mel_spec = np.pad(log_mel_spec, ((0, 0), (0, pad_width)))
        
        return log_mel_spec.T  # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è (194, 40)
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path}: {e}")
        return None

def test_model_on_files(model, file_paths, expected_label, label_name):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Å–ø–∏—Å–∫–µ —Ñ–∞–π–ª–æ–≤"""
    print(f"\nüéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {label_name} –¥–∞–Ω–Ω—ã—Ö:")
    print(f"üìÅ –§–∞–π–ª–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {len(file_paths)}")
    
    results = []
    
    for i, file_path in enumerate(file_paths[:5]):  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 5 —Ñ–∞–π–ª–æ–≤
        print(f"  üìÑ {i+1}/5: {os.path.basename(file_path)}")
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞—É–¥–∏–æ
        spectrogram = preprocess_audio_proper(file_path)
        if spectrogram is None:
            continue
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏
        input_data = np.expand_dims(spectrogram, axis=0)  # –î–æ–±–∞–≤–ª—è–µ–º batch dimension
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        try:
            prediction = model.predict(input_data, verbose=0)
            confidence = float(prediction[0][0])
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å
            predicted_class = 1 if confidence > 0.5 else 0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å
            is_correct = predicted_class == expected_label
            
            results.append({
                'file': os.path.basename(file_path),
                'confidence': confidence,
                'predicted': predicted_class,
                'expected': expected_label,
                'correct': is_correct
            })
            
            status = "‚úÖ" if is_correct else "‚ùå"
            print(f"    {status} Confidence: {confidence:.3f}, Predicted: {predicted_class}, Expected: {expected_label}")
            
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
    
    return results

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ –ü–†–ê–í–ò–õ–¨–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò –ù–ê –ü–†–ò–ú–ï–†–ê–• –î–ê–ù–ù–´–•")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = create_model()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
    weights_loaded = load_trained_weights(model)
    
    if not weights_loaded:
        print("‚ö†Ô∏è –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –±–µ–∑ –æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤")
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
    positives_dir = paths.POSITIVES_RAW
    negatives_dir = paths.NEGATIVES_RAW
    
    print(f"üìÅ –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {positives_dir}")
    print(f"üìÅ –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {negatives_dir}")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–∫–∏ —Ñ–∞–π–ª–æ–≤
    positive_files = glob.glob(os.path.join(positives_dir, "*.wav"))[:5]
    negative_files = glob.glob(os.path.join(negatives_dir, "*.wav"))[:5]
    
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(positive_files)}")
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(negative_files)}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    positive_results = test_model_on_files(model, positive_files, 1, "–ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    negative_results = test_model_on_files(model, negative_files, 0, "–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
    print("=" * 60)
    
    # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if positive_results:
        correct_positives = sum(1 for r in positive_results if r['correct'])
        total_positives = len(positive_results)
        avg_confidence_positives = np.mean([r['confidence'] for r in positive_results])
        
        print(f"‚úÖ –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
        print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ: {correct_positives}/{total_positives} ({correct_positives/total_positives*100:.1f}%)")
        print(f"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence_positives:.3f}")
    
    # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if negative_results:
        correct_negatives = sum(1 for r in negative_results if r['correct'])
        total_negatives = len(negative_results)
        avg_confidence_negatives = np.mean([r['confidence'] for r in negative_results])
        
        print(f"‚ùå –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
        print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ: {correct_negatives}/{total_negatives} ({correct_negatives/total_negatives*100:.1f}%)")
        print(f"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence_negatives:.3f}")
    
    # –û–±—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    all_results = positive_results + negative_results
    if all_results:
        total_correct = sum(1 for r in all_results if r['correct'])
        total_tests = len(all_results)
        overall_accuracy = total_correct / total_tests * 100
        
        print(f"\nüéØ –û–ë–©–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {total_correct}/{total_tests} ({overall_accuracy:.1f}%)")
        
        # –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        positive_confidences = [r['confidence'] for r in positive_results]
        negative_confidences = [r['confidence'] for r in negative_results]
        
        if positive_confidences:
            print(f"üìà –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ: –º–∏–Ω={min(positive_confidences):.3f}, –º–∞–∫—Å={max(positive_confidences):.3f}")
        if negative_confidences:
            print(f"üìâ –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ: –º–∏–Ω={min(negative_confidences):.3f}, –º–∞–∫—Å={max(negative_confidences):.3f}")
        
        # –ê–Ω–∞–ª–∏–∑ –¥–ª—è wake word
        print(f"\nüé§ –ê–ù–ê–õ–ò–ó –î–õ–Ø WAKE WORD:")
        print(f"   FRR (False Reject Rate): {len([r for r in positive_results if not r['correct']])/len(positive_results)*100:.1f}%")
        print(f"   FAR (False Accept Rate): {len([r for r in negative_results if not r['correct']])/len(negative_results)*100:.1f}%")

if __name__ == "__main__":
    main()